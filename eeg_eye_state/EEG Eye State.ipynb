{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores use of an RNN with the [EEG eye state dataset](https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State).\n",
    "> All data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data.\n",
    "\n",
    "As the data is provided in chronological order we can generate labeled sequences suitable for training an RNN.  We'll start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input consists of 14980 samples with 14 dimensions.  Two samples follow:\n",
      "[[4329.23 4009.23 4289.23 4148.21 4350.26 4586.15 4096.92 4641.03 4222.05\n",
      "  4238.46 4211.28 4280.51 4635.9  4393.85]\n",
      " [4324.62 4004.62 4293.85 4148.72 4342.05 4586.67 4097.44 4638.97 4210.77\n",
      "  4226.67 4207.69 4279.49 4632.82 4384.1 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data file\n",
    "# There are 19 rows of \"header\" material in our data file\n",
    "rawdata = np.loadtxt('EEG_eye_state.txt', delimiter=',', skiprows=19, dtype=np.float32)\n",
    "\n",
    "# Separate into X inputs and y labels\n",
    "X_raw = rawdata[:,:-1]\n",
    "y_raw = rawdata[:,-1]\n",
    "print('Input consists of {} samples with {} dimensions.  Two samples follow:'.format(X_raw.shape[0], X_raw.shape[1]))\n",
    "print(X_raw[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample consists of 14 features which are high value numbers which little change through the samples.   These changes are quite small objectively so, to accentuate the patterns present in the miniscule variations in the data we'll need to do some pre-processing so the variations become more pronounced and easier for our model to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scaling and normalizing the data all features are in the range -1 to 1:\n",
      "[[ 0.00293431 -0.01170681  0.56740373 -0.00320852  0.24522723 -0.01978841\n",
      "  -0.00293001  0.85256433  0.00150876  0.18774603  0.2335118   0.0307404\n",
      "   0.01712686 -0.00383413]\n",
      " [ 0.0010844  -0.1120505   0.67139894 -0.00311072  0.00889115 -0.01961054\n",
      "  -0.00281697  0.78225505 -0.00377129 -0.12210524  0.13849996  0.00619956\n",
      "   0.01457776 -0.00548927]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize, scale\n",
    "\n",
    "# normalizing first, then scaling (not the reverse) has a significant effect in the ability for the model to converge\n",
    "X_raw = normalize(X_raw, axis=0)\n",
    "X_raw = scale(X_raw, axis=0)\n",
    "\n",
    "print('After scaling and normalizing the data all features are in the range -1 to 1:')\n",
    "print(X_raw[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been pre-processed we need to convert the chronological list of single EEG observations into samples that consist of **sequences** of EEG observations.   We'll choose a `window_size` which indicates the length of the EEG sequence we want in each sample.   We'll take the first `window_size` EEG observations and turn them into a sequence.  The label for this sequence will be the label that applies to the final  EEG observation in the sequence.  \n",
    "\n",
    "Our next sample will start with the 2nd EEG observation from the previous sample and add one new EEG observation.  The label for this next sample will be the label of the new EEG observation (last in the sequence) added for this sample.  We end up with `window_size` *fewer* samples because the first `window_size` - 1 observations are present in the first sample's sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14970, 10, 14)\n"
     ]
    }
   ],
   "source": [
    "# transform from single labeled EEG reading in to labeled *sequences* of EEG readings\n",
    "def ordered_samples_to_sequence_samples(samples, labels, window_size):\n",
    "    seq_labels = labels[window_size:]\n",
    "    seq_samples = []\n",
    "    for sample_idx in range(window_size, len(samples)):\n",
    "        seq_samples.append(samples[sample_idx - window_size:sample_idx])\n",
    "            \n",
    "    return (np.array(seq_samples), np.array(seq_labels))\n",
    "       \n",
    "window_size = 10\n",
    "X, y = ordered_samples_to_sequence_samples(X_raw, y_raw, window_size)\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the `train_test_split` function to divide into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11227, 10, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_25 (GRU)                 (None, 256)               208128    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 208,385\n",
      "Trainable params: 208,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Activation, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(GRU(256, dropout=0.2, recurrent_dropout=0.2, input_shape=X_train.shape[1:])) # input shape is the 2nd and 3rd dim of data\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.003), # optimizers.Adam(lr=0.003)\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11227/11227 [==============================] - 26s 2ms/step - loss: 0.6639 - acc: 0.5785\n",
      "Epoch 2/50\n",
      "11227/11227 [==============================] - 22s 2ms/step - loss: 0.6398 - acc: 0.6100\n",
      "Epoch 3/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.6085 - acc: 0.6547\n",
      "Epoch 4/50\n",
      "11227/11227 [==============================] - 22s 2ms/step - loss: 0.5704 - acc: 0.6946\n",
      "Epoch 5/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.5474 - acc: 0.7176\n",
      "Epoch 6/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.5112 - acc: 0.7372\n",
      "Epoch 7/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.4873 - acc: 0.7607\n",
      "Epoch 8/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.4620 - acc: 0.7707\n",
      "Epoch 9/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.4416 - acc: 0.7884\n",
      "Epoch 10/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.4323 - acc: 0.7917\n",
      "Epoch 11/50\n",
      "11227/11227 [==============================] - 22s 2ms/step - loss: 0.4024 - acc: 0.8159\n",
      "Epoch 12/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.3961 - acc: 0.8154\n",
      "Epoch 13/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.3805 - acc: 0.8259\n",
      "Epoch 14/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3624 - acc: 0.8374\n",
      "Epoch 15/50\n",
      "11227/11227 [==============================] - 23s 2ms/step - loss: 0.3645 - acc: 0.8354\n",
      "Epoch 16/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3562 - acc: 0.8434\n",
      "Epoch 17/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3449 - acc: 0.8489\n",
      "Epoch 18/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3334 - acc: 0.8536\n",
      "Epoch 19/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3243 - acc: 0.8584\n",
      "Epoch 20/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3290 - acc: 0.8586\n",
      "Epoch 21/50\n",
      "11227/11227 [==============================] - 25s 2ms/step - loss: 0.3178 - acc: 0.8606\n",
      "Epoch 22/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3126 - acc: 0.8636\n",
      "Epoch 23/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.3076 - acc: 0.8671\n",
      "Epoch 24/50\n",
      "11227/11227 [==============================] - 25s 2ms/step - loss: 0.3008 - acc: 0.8752: 0s - loss: 0.3014 - acc: 0.\n",
      "Epoch 25/50\n",
      "11227/11227 [==============================] - 25s 2ms/step - loss: 0.2959 - acc: 0.8740\n",
      "Epoch 26/50\n",
      "11227/11227 [==============================] - 25s 2ms/step - loss: 0.2908 - acc: 0.8702\n",
      "Epoch 27/50\n",
      "11227/11227 [==============================] - 24s 2ms/step - loss: 0.2941 - acc: 0.8751\n",
      "Epoch 28/50\n",
      "11227/11227 [==============================] - 22s 2ms/step - loss: 0.2862 - acc: 0.8782\n",
      "Epoch 29/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2837 - acc: 0.8786\n",
      "Epoch 30/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2746 - acc: 0.8817\n",
      "Epoch 31/50\n",
      "11227/11227 [==============================] - 14s 1ms/step - loss: 0.2820 - acc: 0.8821\n",
      "Epoch 32/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2696 - acc: 0.8898\n",
      "Epoch 33/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2571 - acc: 0.8891\n",
      "Epoch 34/50\n",
      "11227/11227 [==============================] - 11s 983us/step - loss: 0.2784 - acc: 0.8809\n",
      "Epoch 35/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2643 - acc: 0.8872\n",
      "Epoch 36/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2656 - acc: 0.8887\n",
      "Epoch 37/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2614 - acc: 0.8889\n",
      "Epoch 38/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2560 - acc: 0.8910\n",
      "Epoch 39/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2429 - acc: 0.8979\n",
      "Epoch 40/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2455 - acc: 0.8961\n",
      "Epoch 41/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2384 - acc: 0.9022\n",
      "Epoch 42/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2563 - acc: 0.8918\n",
      "Epoch 43/50\n",
      "11227/11227 [==============================] - 14s 1ms/step - loss: 0.2482 - acc: 0.8999\n",
      "Epoch 44/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2386 - acc: 0.9026\n",
      "Epoch 45/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2373 - acc: 0.9041\n",
      "Epoch 46/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2326 - acc: 0.8998\n",
      "Epoch 47/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2403 - acc: 0.9031\n",
      "Epoch 48/50\n",
      "11227/11227 [==============================] - 13s 1ms/step - loss: 0.2351 - acc: 0.9030\n",
      "Epoch 49/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2358 - acc: 0.9031\n",
      "Epoch 50/50\n",
      "11227/11227 [==============================] - 12s 1ms/step - loss: 0.2383 - acc: 0.9026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1765bf8a20>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3743/3743 [==============================] - 2s 535us/step\n",
      "Test score: 0.11005482444255517\n",
      "Test accuracy: 0.9706118087095912\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=32)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: setup callbacks - https://keras.io/callbacks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
